name: Code Review Assistant

# Run on both same-repo PRs and forks safely.
# pull_request_target gives write perms to comment on forked PRs,
# but we must CHECK OUT THE PR HEAD READ-ONLY to avoid executing untrusted code.
on:
  pull_request:
    types: [opened, reopened, synchronize]
  pull_request_target:
    types: [opened, reopened, synchronize]

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  review:
    runs-on: ubuntu-latest
    steps:
      - name: Determine event & ref
        id: meta
        run: |
          echo "event=${{ github.event_name }}" >> $GITHUB_OUTPUT
          # On pull_request_target, the default checkout is the BASE repo.
          # We'll fetch the PR HEAD ref explicitly (read-only) to analyze code.
          echo "repo=${{ github.event.pull_request.head.repo.full_name }}" >> $GITHUB_OUTPUT
          echo "ref=${{ github.event.pull_request.head.ref }}" >> $GITHUB_OUTPUT

      # Checkout the PR HEAD in read-only mode (no token write perms used to fetch)
      - name: Checkout PR HEAD safely
        uses: actions/checkout@v4
        with:
          repository: ${{ steps.meta.outputs.repo }}
          ref: ${{ steps.meta.outputs.ref }}
          fetch-depth: 2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ./app/requirements.txt

      - name: Start Ollama in Docker
        run: |
          docker run -d --name ollama -p 11434:11434 ghcr.io/ollama/ollama:latest
          # wait for API to be up
          timeout 150s bash -c 'until curl -fsS http://127.0.0.1:11434/api/tags >/dev/null; do sleep 3; done'
          echo "✅ Ollama container is up"

      - name: Pull model (HTTP API) and wait
        env:
          LLM_MODEL: qwen2.5-coder:7b-instruct
        run: |
          # ask Ollama to pull the model
          curl -fsS http://127.0.0.1:11434/api/pull -d "{\"name\":\"$LLM_MODEL\"}"
          # wait until model shows up in /api/tags
          timeout 600s bash -c 'until curl -fsS http://127.0.0.1:11434/api/tags | grep -q "$LLM_MODEL"; do sleep 5; done'
          echo "LLM_MODEL=$LLM_MODEL" >> "$GITHUB_ENV"

      - name: Run analyzers + LLM review and post PR comment
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          LLM_MODEL: ${{ env.LLM_MODEL }}
          PYTHONPATH: .
        run: |
          python app/scripts/ci_review.py || {
            echo "❌ ci_review.py failed, dumping recent logs"; 
            tail -n 200 /tmp/ollama.log || true;
            exit 1;
          }
